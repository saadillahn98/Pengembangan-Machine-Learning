# -*- coding: utf-8 -*-
"""Project Pengembangan Machine Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g2pwSkRBfqyi9N361cdrRydwEJc2-ppa

**Merumuskan Masalah Dalam MACHINE LEARNING**

Masalah yang saya rumuskan pada model ini yaitu klasifikasi dan regresi, ada beberapa subtipe dari 2 masalah tersebut. Flowchart di bawah dapat menunjukkan bagaimana menentukan subtipe dari masalah klasifikasi dan regresi. Dengan flowchart ini, kita dapat lebih mudah mengkomunikasikan masalah ML kita dengan pengembang lain. 

Masalah tersebut terbagi menjadi :
A. Klasifikasi biner
B. Regresi satu dimensi
C. Klasifikasi banyak-kelas, label-tunggal
D. Klasifikasi banyak-kelas, banyak-label
E. Regresi multidimensi
F. Pengklasteran
G. Lainnya (penerjemahan, bounding box, dsb)

**Pendahuluan : Proses Membuat dan Melatih Model Neural Network dengan Tensorflow dan Keras**

Pada modul ini kita akan membahas lebih dalam lagi proses pembuatan model tersebut dan teknik-lain yang dapat diterapkan untuk meningkatkan performa model kita seperti:

A. Membuat plot akurasi dan loss dari model
B. Menggunakan Callback
C. Batch loading
E. Dropout

**Pra Pemrosesan Data untuk Model**

Secara umum, ada 4 tahapan yang perlu kita lakukan dalam memproses data yaitu:

A. Ubah dataset ke dalam bentuk larik. Yup, larik berisi angka-angka adalah format data yang dapat diterima oleh model kita. Masih ingat bukan, seperti pada kelas belajar machine learning untuk pemula, model kita menerima gambar sebagai matriks atau larik 2 dimensi.
B. Pisahkan atribut dan label pada data. Model kita akan mempelajari korelasi antara atribut dan label pada dataset kita.
Ubah skala data dalam skala yang seragam. Tahap ini sudah kita pelajari sebelumnya. Masih ingat? Nama teknik ini adalah normalization. Normalization dilakukan karena NN bekerja dengan baik memproses nilai yang berada di antara 0 dan 1.
C. Terakhir, pisahkan dataset ke dalam data latih dan data uji. Betul, kita memerlukan data uji untuk mengevaluasi kinerja dari model yang telah kita latih ya.

**Menggunakan Model Untuk Melakukan Prediksi**
"""

import tensorflow as tf
import numpy as np
from tensorflow import keras

xs = np.array([1.0,  2.0, 3.0, 4.0, 5.0, 6.0], dtype=float)
ys = np.array([4.0, 6.0, 8.0, 10.0, 12.0, 14.0], dtype=float)

model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])

"""Pada model sequential ini, kita kemudian isi layer yang kita inginkan untuk model kita. Untuk membuat sebuah layer, kita dapat menggunakan fungsi keras.layers.Dense().

Parameter units dari fungsi keras.layers.Dense() adalah jumlah perseptron yang dimiliki oleh layer tersebut. Yang perlu diperhatikan pada model sequential adalah, layer pertama dari model tersebut haruslah memiliki parameter input_shape agar model bisa mengenali bentuk input yang akan diprosesnya.
Parameter input_shape menunjukkan bentuk dari setiap elemen input yang akan diterima oleh model. Pada kasus kita setiap elemen dari data kita adalah sebuah bilangan numerik 1 digit, sehingga kita bisa isi input_shape kita dengan angka 1. Jika sebuah elemen dari dataset kita berupa gambar yang memiliki dimensi 32*32 piksel, maka input_shape yang sesuai adalah [32,32].

Kemudian hal yang paling penting adalah menentukan optimizer dan loss dari model agar model kita bisa belajar. Untuk menentukan optimizer dan loss kita gunakan fungsi compile. Untuk masalah regresi kita yang sederhana, kita dapat menggunakan stochastic gradient descent sebagai optimizer, dan mean squared error sebagai loss function model kita
"""

model.compile(optimizer='sgd', loss='mean_squared_error')

"""Terakhir, yuk kita panggil fungsi yang paling terkenal dari machine learning yaitu fit(). **Fungsi fit()** adalah fungsi di mana kita menyuruh model kita untuk mempelajari hubungan antara atribut dan label pada dataset."""

model.fit(xs, ys, epochs=150)

"""Ketika model kita telah dilatih, kita kemudian dapat menggunakan model tersebut untuk memprediksi data yang belum pernah dilihatnya menggunakan fungsi predict. Ketika kita menjalankan kode di bawah, hasil yang kita dapat seharusnya adalah 22 dimana 2(10)+2=22"""

print(model.predict([10.0]))

"""Menariknya, hasil yang didapat dari JST kita akan mendekati angka 22. Kenapa demikian? Karena JST menghitung probabilitas. Neural network tidak memprediksi kepastian melainkan probabilitas. Pada kasus kita neural network mempelajari bahwa pola yang terdapat pada dataset kemungkinan adalah 2X+2, namun ia juga belum pasti tentang hal itu. Sehingga prediksi yang dihasilkan adalah probabilitas hasilnya mendekati 22. Semakin banyak data yang kita berikan dan dan juga eror ketika training model semakin kecil, maka prediksi dari NN akan semakin mendekati 22

**Model Sekuensial dengan Beberapa Layer**
"""

model = tf.keras.Sequential([
                             keras.layers.Dense(units=1, input_shape=[1]),
                             keras.layers.Dense(units=8),
                             keras.layers.Dense(units=1)])

"""**Membuat Model untuk Klasifikasi Dua Kelas**

Nah, pada Dataset ini kita akan coba melatih model untuk mengklasifikasikan apakah sebuah buah merupakan jeruk atau anggur.
"""

import pandas as pd
df = pd.read_csv('citrus.csv')

"""Kemudian untuk melihat overview dari dataset kita bisa memanggil fungsi info() pada dataframe"""

df.info()

"""Selanjutnya gunakan fungsi head() untuk menampilkan dataframe kita. Pada output dari cell di bawah, label dari dataset adalah kolom pertama yang berisi string ‘orange’ dan ‘grapefruit’. """

df.head(10000)

"""Selanjutnya mengubah nilai-nilai pada kolom name menjadi bilangan numerik. Untuk nilai yang berisi string ‘orange’ akan diubah menjadi 0 dan nilai yang berisi string ‘grapefruit’ akan diubah menjadi 1."""

df.name[df.name == 'orange'] = 0
df.name[df.name == 'grapefruit'] = 1

"""Sampai pada tahap ini model kita belum dapat memproses dataset ini karena dataset kita masih dalam bentuk dataframe. Betul, dataset harus dalam bentuk array agar dapat diproses oleh model. Nah untungnya kita dapat melakukan ini dengan mudah menggunakan atribut values dari dataframe. Values mengembalikan numpy array yang dikonversi dari dataframe."""

dataset = df.values
dataset

#Memisahkan atribut dan label pada dataset

# pilih 5 kolom terakhir sebagai atribut
X = dataset[:,1:6]
# bilangan sebelum koma untuk memilih baris pada dataframe
# bilangan setelah koma untuk memilih kolom pada dataframe

y = dataset[:,0]

# Normalization
from sklearn import preprocessing
min_max_scaler = preprocessing.MinMaxScaler()
X_scale = min_max_scaler.fit_transform(X)
X_scale

#Memisahkan kembali data menjadi data latih dan data uji

# pisahkan data training dan testing
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X_scale, y, test_size=0.3)

#Mengubah tipe data menjadi float 32

import numpy as np
 
Y_train = Y_train.astype(np.float32)
Y_test = Y_test.astype(np.float32)

"""Tahapan sudah selesai, saatnya membangun Model JST dari Dataset yang kita miliki"""

from keras.models import Sequential
from keras.layers import Dense

model = Sequential([    
                    Dense(32, activation='relu', input_shape=(5,)),    
                    Dense(32, activation='relu'),    
                    Dense(1, activation='sigmoid'),])

#Menampilkan akurasi dengan menambahkan parameter metriks dan mengisi dengan string "accuracy"

model.compile(optimizer='sgd',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.fit(X_train, Y_train, epochs=100)

#Evaluasi model dengan fungsi evaluate

model.evaluate(X_test, Y_test)
# elemen pertama adalah loss dan elemen kedua adalah akurasi

"""**Membuat dan Melatih Model untuk Klasifikasi Banyak Kelas**"""

#Import library dari data yang kita masukkan

import pandas as pd
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense

df = pd.read_csv('Iris.csv')
df

#Membuang kolom dengan fungsi drop()

df = df.drop(columns='Id')

#Memerlukan one hot encoding

category = pd.get_dummies(df.Species)
category

#Membuang kolom spesies 

new_df = pd.concat([df, category], axis=1)
new_df = new_df.drop(columns='Species')
new_df

#Konversi datafram menjadi numpy array

dataset = new_df.values
dataset

#Memisahkan atribut dan label

# Pilih 4 kolom pertama untuk dijadikan sebagai atribut
X = dataset[:,0:4]
# Pilih 3 kolom terakhir sebagai label
y = dataset[:,4:7]

#Menormalisasi data

# Normalize
min_max_scaler = preprocessing.MinMaxScaler()
X_scale = min_max_scaler.fit_transform(X)
X_scale

#Membagi data menjadi data latih dan data uji

X_train, X_test, Y_train, Y_test = train_test_split(X_scale, y, test_size=0.3)

#Arsitektur model menggunakan 3 layers

model = Sequential([    
                    Dense(64, activation='relu', input_shape=(4,)),    
                    Dense(64, activation='relu'),    
                    Dense(3, activation='softmax'),])

#Menentukan optimzer loss dan function dari model

model.compile(optimizer='Adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

#Memanggil fungsi fit()

hist = model.fit(X_train, Y_train, epochs=100)

#Melihat akurasi prediksi model dari dataset

model.evaluate(X_test, Y_test)

"""**Plot Loss dan Akurasi dari Trained Model**"""

#Import Library Matplotlib

import matplotlib.pyplot as plt

plt.plot(hist.history['loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='upper right')
plt.show()

#Mengecek akurasi model stagnan pada epoch disekitar 20 dan diatas 80

plt.plot(hist.history['accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='lower right')
plt.show()

"""**Mencegah Overfitting dengan Dropout**"""

model = tf.keras.models.Sequential([
                                    tf.keras.layers.Conv2D(64, (3, 3), activation='relu',
                                                           input_shape=(150, 150, 3)),
                                    tf.keras.layers.MaxPooling2D(2, 2),
                                    tf.keras.layers.Flatten(),
                                    tf.keras.layers.Dense(512, activation='relu'),
                                    tf.keras.layers.Dropout(0.2),
                                    tf.keras.layers.Dense(256, activation='relu'),
                                    tf.keras.layers.Dropout(0.5),
                                    tf.keras.layers.Dense(6, activation='softmax')
])

"""**Penggunaan Callbacks**"""

import tensorflow as tf
import pandas as pd
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense
df = pd.read_csv('Iris.csv')
df = df.drop(columns='Id')
category = pd.get_dummies(df.Species)
new_df = pd.concat([df, category], axis=1)
new_df = new_df.drop(columns='Species')
dataset = new_df.values
X = dataset[:,0:4]
y = dataset[:,4:7]
min_max_scaler = preprocessing.MinMaxScaler()
X_scale = min_max_scaler.fit_transform(X)
X_train, X_test, Y_train, Y_test = train_test_split(X_scale, y, test_size=0.3)
model = Sequential([    
                    Dense(64, activation='relu', input_shape=(4,)),    
                    Dense(64, activation='relu'),    
                    Dense(3, activation='sigmoid'),])
model.compile(optimizer='Adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

#Menulis kode print(“\nAkurasi telah mencapai >90%!”)

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9):
      print("\nAkurasi telah mencapai >90%!")
      self.model.stop_training = True
callbacks = myCallback()

#Melihat parameter callbacks

model.fit(X_train, Y_train, epochs=50, callbacks=[callbacks])

"""**Menggunakan Dataset dari tf.data.datasets**"""

#Import library Tensorflow

import tensorflow as tf
print(tf.__version__)

#Memasukkan fungsi tf.keras.dataset

mnist = tf.keras.datasets.mnist

(gambar_latih, label_latih), (gambar_testing, label_testing) = mnist.load_data()

#Menampilkan label dari satu gambar 

import numpy as np
np.set_printoptions(linewidth=200)
import matplotlib.pyplot as plt
plt.imshow(gambar_latih[0])
print(label_latih[0])

#Menormalisasi gambar

gambar_latih  = gambar_latih / 255.0
gambar_testing = gambar_testing / 255.0

#Memasukkan layer Flatten
model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)), 
                                    tf.keras.layers.Dense(128, activation=tf.nn.relu), 
                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])

#Menentukan optimizer dan loss
model.compile(optimizer = tf.optimizers.Adam(),
              loss = 'sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.fit(gambar_latih, label_latih, epochs=5)

"""**Penggunaan Batch Loading**"""

import tensorflow as tf
mnist = tf.keras.datasets.mnist
(training_images, training_labels), (test_images, test_labels) = mnist.load_data()
training_images  = training_images / 255.0
test_images = test_images / 255.0
model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)), 
                                    tf.keras.layers.Dense(128, activation=tf.nn.relu), 
                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])
model.compile(optimizer = tf.optimizers.Adam(),
              loss = 'sparse_categorical_crossentropy',
              metrics=['accuracy'])

#Memasukkan parameter batch size
model.fit(training_images, training_labels, batch_size=32, epochs=5)

#Mengubah batch size lebih besar
model.fit(training_images, training_labels, batch_size=128, epochs=5)

"""**Pendahuluan Sistem Rekomendasi**

Sistem rekomendasi sudah sangat sering kita jumpai sehari-hari, mulai dari membantu kita menemukan pekerjaan, restoran, film, berita, bahkan hingga jodoh seperti pada Tinder.
Materi yang akan dibahas
A. Apa itu sistem rekomendasi
B. Content bases filtering
C. Collaborative filtering
D. Sistem rekomendasi dengan jaringan saraf tiruan

**Membuat dan Melatih Model untuk Memproses Dataset Gambar dari Dunia Nyata**
"""

#Memprediksi dataset chessman

!wget --no-check-certificate \
    https://github.com/dicodingacademy/assets/raw/main/ml_pengembangan_academy/Chessman-image-dataset.zip \
    -O /tmp/Chessman-image-dataset.zip

#Ekstrak data dalam bentuk File Zip

import os
import zipfile
local_zip = '/tmp/Chessman-image-dataset.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp')
zip_ref.close()

#Memanggil fungsi Listdir

os.listdir('/tmp/Chessman-image-dataset/Chess')

#Memasukkan augmentasi gambar

print('total pawn images :', len(os.listdir('/tmp/Chessman-image-dataset/Chess/Pawn')))
print('total King images :', len(os.listdir('/tmp/Chessman-image-dataset/Chess/King')))
print('total Knight images :', len(os.listdir('/tmp/Chessman-image-dataset/Chess/Knight')))

# Commented out IPython magic to ensure Python compatibility.
#Melihat gambar 
from keras.preprocessing import image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
# %matplotlib inline
img = image.load_img('/tmp/Chessman-image-dataset/Chess/King/00000145.png')
imgplot = plt.imshow(img)

#Import image data generator
from tensorflow.keras.preprocessing.image import ImageDataGenerator
 
 
train_dir = os.path.join('/tmp/Chessman-image-dataset/Chess')
train_datagen = ImageDataGenerator(rescale=1./255,
    rotation_range=20,
    zoom_range=0.2,
    shear_range=0.2,
    fill_mode = 'nearest',
    validation_split=0.1) # set validation split

#Menambahkan parameter subset dengan nilai training

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(150, 150),
    batch_size=8,
    class_mode='categorical',
    subset='training') # set as training data
validation_generator = train_datagen.flow_from_directory(
    train_dir, # same directory as training data
    target_size=(150, 150),
    batch_size=16,
    class_mode='categorical',
    subset='validation')

#Membuat arsitektur model 

import tensorflow as tf
model = tf.keras.models.Sequential([
    # Note the input shape is the desired size of the image 150x150 with 3 bytes color
    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Dropout(0.4),  
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'), 
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Dropout(0.4),  
    # Flatten the results to feed into a DNN
    tf.keras.layers.Flatten(), 
    # 512 neuron hidden layer
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(6, activation='softmax')  
])

#Menentukan optimizer dan loss

model.compile(optimizer=tf.optimizers.Adam(),
              loss='categorical_crossentropy',
              metrics = ['accuracy'])

#Memanggil fungsi fit()

history = model.fit(train_generator,
                              validation_data=validation_generator,
                              epochs=50,
                              verbose=2)

#Membuat plot akurasi

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Akurasi Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

#Membuat plot loss

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""**Transfer Learning**

Ide dibalik transfer learning untuk masalah computer vision adalah, model yang telah dilatih pada dataset berukuran besar yang berisi gambar umum mampu dipakai sebagai model dasar yang membantu kita untuk mengenali fitur/bentuk-bentuk benda yang terdapat di dunia nyata.

Untuk melihat bagaimana efektifnya transfer learning, kita akan belajar menggunakan transfer learning pada dataset cheesman yang kita pelajari pada submodul sebelumnya.

Kode yang akan kita gunakan pada submodul ini hampir sama dengan kode pada latihan sebelumnya, jadi silahkan menyalin atau menggunakan kode pada latihan sebelumnya.
"""

#Unduh dataset cheesman

!wget --no-check-certificate \
  https://github.com/dicodingacademy/assets/raw/main/ml_pengembangan_academy/Chessman-image-dataset.zip \
  -O /tmp/Chessman-image-dataset.zip

#Membuat direktori pada ImageDataGenerator

import os
import zipfile
from tensorflow.keras.preprocessing.image import ImageDataGenerator
local_zip = '/tmp/Chessman-image-dataset.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp')
zip_ref.close()
train_dir = os.path.join('/tmp/Chessman-image-dataset/Chess')
train_datagen = ImageDataGenerator(rescale=1./255,
    rotation_range=20,
    zoom_range=0.2,
    shear_range=0.2,
    fill_mode = 'nearest',
    validation_split=0.1) # set validation split

#Membagi dataset menjadi data training dan data validasi

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(150, 150),
    batch_size=8,
    class_mode='categorical',
    subset='training') # set as training data
validation_generator = train_datagen.flow_from_directory(
    train_dir, # same directory as training data
    target_size=(150, 150),
    batch_size=16,
    class_mode='categorical',
    subset='validation')

#Memanggil kelas ResNet152V2 dan mengisi parameter 

import tensorflow as tf
from tensorflow.keras.layers import Input
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications import ResNet152V2
model = tf.keras.models.Sequential([
    ResNet152V2(weights="imagenet", include_top=False, input_tensor=Input(shape=(150, 150, 3))),
    # tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Flatten(), 
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(6, activation='softmax')  
])
model.layers[0].trainable = False

#Menentukan optimizer, loss, serta metrik

model.compile(optimizer=tf.optimizers.Adam(),
              loss='categorical_crossentropy',
              metrics = ['accuracy'])

#Melakukan pelatihan model

history = model.fit(train_generator,
                              validation_data=validation_generator,
                              epochs=50,
                              verbose=2)

"""**Style Transfer**"""

#Mengubah tensor menjadi gambar 

import tensorflow as tf
def tensor_to_image(tensor):
  tensor = tensor*255
  tensor = np.array(tensor, dtype=np.uint8)
  if np.ndim(tensor)>3:
    assert tensor.shape[0] == 1
    tensor = tensor[0]
  return PIL.Image.fromarray(tensor)

#Mengubah gambar menjadi tensor

def image_to_tensor(path_to_img):
    img = tf.io.read_file(path_to_img)
    img = tf.image.decode_image(img, channels=3, dtype=tf.float32)
    
    # Resize the image to specific dimensions
    img = tf.image.resize(img, [512, 512])
    img = img[tf.newaxis, :]
    return img

#Konversi gambar yang di unduh

kucing_tensor = image_to_tensor('/content/kucing.jpg')
style_tensor = image_to_tensor('/content/style.jpg')

#Menampilkan hasil gambar pada style transfer

import tensorflow_hub as hub
import numpy as np
import PIL.Image
import time
import functools
hub_module = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/1')
stylized_image = hub_module(tf.constant(kucing_tensor), tf.constant(style_tensor))[0]
tensor_to_image(stylized_image)

"""**Implementasi Natural Language Processing (NLP)**

Berbeda dengan pemrosesan gambar yang telah kita pelajari, pemrosesan teks memiliki tantangan tersendiri seperti perbedaan panjang teks, bahasa, serta bagaimana merepresentasikan teks ke dalam format yang dapat diterima oleh sebuah model.

Materi yang akan dibahas pada modul ini antara lain:

Penggunaan Tokenizer
Implementasi Embedding
Layer LSTM
Klasifikasi teks 2 kelas dan multi-kelas

**Latihan Tokenization**
"""

#Import library

from tensorflow.keras.preprocessing.text import Tokenizer

#Menambahkan parameter OOV

tokenizer = Tokenizer(num_words= 15, oov_token='-')

#Membuat teks yang akan di Tokenisasi

teks = ['Saya suka programming',
        'Programming sangat menyenangkan!',
        'Machine Learning berbeda dengan pemrograman konvensional']

#Memanggil fungsi fit_on_text()

tokenizer.fit_on_texts(teks)

#Mengubah teks dengan fungsi text_to_sequences

sequences = tokenizer.texts_to_sequences(teks)

#Melihat hasil Tokenisasi

print(tokenizer.word_index)

#Menggunakan fungsi OOV

print(tokenizer.texts_to_sequences(['Saya suka programming']))
print(tokenizer.texts_to_sequences(['Saya suka belajar programming sejak SMO']))

#Menggunakan padding dengan library pad_sequence

from tensorflow.keras.preprocessing.sequence import pad_sequences
sequences_samapanjang = pad_sequences(sequences)

#Print sequence sama panjang

print(sequences_samapanjang)

#Mengatur parameter maxlen yang tidak lebih dari 5

sequences_samapanjang = pad_sequences(sequences, 
                                      padding='post',
                                      maxlen=5)

#Parameter truncating dan mengisi nilai post

sequences_samapanjang = pad_sequences(sequences, 
                                      padding='post',
                                      maxlen=5,
                                      truncating='post')

"""**Fungsi Embedding**

Pada klasifikasi teks, kita perlu melakukan embedding yang merupakan kunci dalam klasifikasi teks di Tensorflow. Embedding memungkinkan model ML untuk memahami makna di setiap kata dan mengelompokkan kata-kata dengan makna yang mirip agar berdekatan
"""

#Memanggil fungsi flatten pada Embedding

import io
import os
import re
import shutil
import string
import tensorflow as tf

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D
from tensorflow.keras.layers import TextVectorization

#Memanggil fungsi compile

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

"""**Binary Text Classification**

Untuk melihat data kali ini kita akan menggunakan dataset Yelp yang berisi review dari beberapa restoran di Amerika Serikat. Dataset terdiri dari 2 kelas yaitu 0 dan 1 yang menunjukkan apakah review tersebut positif atau negatif.
"""

#Import data

import pandas as pd
df = pd.read_csv('yelp_labelled.txt', names=['sentence', 'label'], sep='\t')

#Melihat 5 sampel 

df.tail()

#Membagi dataset menjadi train dan test set

from sklearn.model_selection import train_test_split
kalimat = df['sentence'].values
y = df['label'].values
kalimat_latih, kalimat_test, y_latih, y_test = train_test_split(kalimat, y, test_size=0.2)

#Menggunakan fungsi pad_sequence

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
 
tokenizer = Tokenizer(num_words=250, oov_token='x')
tokenizer.fit_on_texts(kalimat_latih) 
tokenizer.fit_on_texts(kalimat_test)
 
sekuens_latih = tokenizer.texts_to_sequences(kalimat_latih)
sekuens_test = tokenizer.texts_to_sequences(kalimat_test)
 
padded_latih = pad_sequences(sekuens_latih) 
padded_test = pad_sequences(sekuens_test)

#Mengganti layer flatten dengan GlobalAveragePooling1D

import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(250, 16, input_length=20),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(24, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

"""**Long Short-Term Memory (LSTM) Layer**

Untuk mengimplementasikan LSTM sendiri sangatlah mudah pada Keras. Kita cukup menambahkan layer LSTM dan mengisi parameter dengan jumlah output yang diinginkan dari layer tersebut. Contoh penggunaan LSTM dapat dilihat pada kode di bawah. Kita dapat meletakkan layer LSTM setelah layer embedding dan sebelum Dense layer.
"""

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(5, activation='softmax')
])

"""**Multiclass Text Classification**

Tujuan kita adalah menentukan genre sebuah film berdasarkan sinopsisnya. Namun sebelumnya dataset kita unduh melalui Kaggle
"""

#Import data yang kita unduh

import pandas as pd
df = pd.read_csv('imdb_indonesian_movies_2.csv')
df = df.drop(columns=['judul_film'])

#Melihat data teratas 

df.head()

#Melakukan proses hot encoding

category = pd.get_dummies(df.genre)
df_baru = pd.concat([df, category], axis=1)
df_baru = df_baru.drop(columns='genre')
df_baru

#Mengubah tipe data menjadi tipe data Array

sinopsis = df_baru['ringkasan_sinopsis'].values
label = df_baru[['Drama', 'Horor', 'Komedi', 'Laga', 'Romantis']].values

#Membagi data training untuk jadi data testing

from sklearn.model_selection import train_test_split
sinopsis_latih, sinopsis_test, label_latih, label_test = train_test_split(sinopsis, label, test_size=0.2)

#Mengkonversi sample menjadi data sequence

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
 
tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(sinopsis_latih) 
tokenizer.fit_on_texts(sinopsis_test)
 
sekuens_latih = tokenizer.texts_to_sequences(sinopsis_latih)
sekuens_test = tokenizer.texts_to_sequences(sinopsis_test)
 
padded_latih = pad_sequences(sekuens_latih) 
padded_test = pad_sequences(sekuens_test)

#Memanggil fungsi compile dan menentukan optimizer

import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(5, activation='softmax')
])
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

#Melatih model dengan fungsi fit()

num_epochs = 30
history = model.fit(padded_latih, label_latih, epochs=num_epochs, 
                    validation_data=(padded_test, label_test), verbose=2)

"""**Time Series**

**Latihan Time Series dengan LSTM**
"""

#Import data 

import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf

#Menampilkan 5 data teratas

data_train = pd.read_csv('DailyDelhiClimateTrain.csv')
data_train.head()

#Mengecek nilai yang hilang

data_train.isnull().sum()

#Memanggil fungsi plot dari Library

dates = data_train['date'].values
temp  = data_train['meantemp'].values
 
 
plt.figure(figsize=(15,5))
plt.plot(dates, temp)
plt.title('Temperature average',
          fontsize=20);

#Mengkonversi menjadi tipe numpy

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

#Menggunakan 2 model Layer LSTM

train_set = windowed_dataset(temp, window_size=60, batch_size=100, shuffle_buffer=1000)
model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(60, return_sequences=True),
  tf.keras.layers.LSTM(60),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(1),
])

#Evaluasi model MAE

optimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
history = model.fit(train_set,epochs=100)

"""**Model Deployment**"""

def predict(data):
  data['is_good'] = data['rating'] > 3
  return model.predict(data)

"""**Format Penyimpanan Model**"""

#Format Save Mode

save_path = 'mymodel/'
tf.saved_model.save(model, save_path)

"""**Format Penyimpanan ONNX**"""

#Install

!pip install onnxruntime
!pip install -U tf2onnx

!python -m tf2onnx.convert --saved-model "saved_model" --output "model.onnx" --extra_opset ai.onnx.contrib:1